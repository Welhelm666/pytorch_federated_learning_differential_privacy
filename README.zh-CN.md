[English](README.md) | [简体中文](README.zh-CN.md)
### **关于此分支**

**本项目是 [rruisong/pytorch_federated_learning](https://github.com/rruisong/pytorch_federated_learning) 的一个分支。**此分支的主要贡献是实现了**客户端差分隐私 (LDP)**，用于研究联邦学习中的隐私-效用权衡。**有关环境设置、训练和评估的详细说明，请参阅原始代码库。**

**主要修改：**
* 在客户端训练逻辑中集成了**拉普拉斯噪声机制**，用于在模型权重上传到服务器之前对其进行扰动。
* 通过 test_config.yaml 文件灵活地实现差异隐私功能，允许用户轻松启用/禁用它并调整噪声强度。
* 进行了系统的比较实验，以量化不同隐私级别对模型性能的影响。
---

### **核心实验成果展示**

本项目在病态非独立同分布（Non-IID, 每客户端2类）的挑战性环境下，对差分隐私的效用进行了评估。下图展示了在不同拉普拉斯噪声强度 (`b`)下，FedAvg算法在MNIST数据集上的性能表现：

![Federated Learning LDP Comparison](<figures/FedAvg_LeNet_MNist_NIID_LDP_Comparison_Annotated.png>)

**实验结论:**

| 噪声强度 (b) | 最高准确率 (Max Acc) | 相比无噪声的准确率下降 | 首次达到最高准确率的轮次 |
| :--- | :--- | :--- | :--- |
| 0.00 (无噪声) | **98.75%** | - | 1962 |
| 0.01 | **98.33%** | 0.42% | 1721 |
| 0.03 | **97.42%** | 1.33% | 1939 |
| 0.05 | **95.56%** | 3.19% | 1575 |

从数据中可以清晰地看到隐私与效用之间的非线性权衡关系。特别地，在噪声强度为0.01时，系统可在仅牺牲0.42%准确率的微小代价下，获得有效的隐私保护，找到了一个实际可行的平衡点。
